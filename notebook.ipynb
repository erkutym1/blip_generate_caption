{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59sFP6ekwp0d"
   },
   "source": [
    "# **1- KAGGLE API + DATASET**\n",
    "BURAYI GOOGLE COLAB ƒ∞√áERƒ∞Sƒ∞NDE KAGGLE API KULLANIMI VE DATASET ƒ∞NDƒ∞RMESƒ∞ ƒ∞√áƒ∞N KULLANDIM.\n",
    "\n",
    "I USED THIS TO USE THE KAGGLE API AND DOWNLOAD DATASETS WITHIN GOOGLE COLAB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "NpM62fvkCj7P"
   },
   "outputs": [],
   "source": [
    "# DATASET EKLEME ALANI\n",
    "# ADD DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPmbLMizwzL-"
   },
   "source": [
    "# **2- K√úT√úPHANELER ve GOOGLE DRIVE**\n",
    "LIBRARIES + GOOGLE DRIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N11Z00YaHdVe"
   },
   "outputs": [],
   "source": [
    "!pip install torchvision timm\n",
    "!pip install transformers accelerate datasets\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "!pip install transformers\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install pillow\n",
    "!pip install tqdm\n",
    "!pip install scikit-learn\n",
    "!pip install nltk\n",
    "\n",
    "!git clone https://github.com/tylin/coco-caption.git\n",
    "!pip install pycocoevalcap\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnZROlwQw29m"
   },
   "source": [
    "## **3- ƒ∞LK Eƒûƒ∞Tƒ∞M**\n",
    "BURADAKƒ∞ KOD, MODEL Eƒûƒ∞Tƒ∞Mƒ∞ ƒ∞√áƒ∞N BA≈ûLANGI√á KODUDUR. ƒ∞LK √ñƒûRENƒ∞MLERƒ∞ BURADA YAPTIM. 10 EPOCH SONRA DEVAM KODUNA GE√áTƒ∞M.\n",
    "\n",
    "THE CODE HERE IS THE STARTING CODE FOR MODEL TRAINING. I DID MY FIRST LEARNINGS HERE. AFTER 10 EPOCHES I SWITCHED TO THE CONTINUING CODE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "SznsNR2q8Cyq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, get_cosine_schedule_with_warmup\n",
    "from torch import optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torchvision.transforms as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.amp import autocast\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def clean_caption(caption):\n",
    "    caption = re.sub(r'[^\\x00-\\x7F]+', ' ', caption)\n",
    "    caption = re.sub(r'\\s+', ' ', caption).strip()\n",
    "    return caption\n",
    "\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, processor):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.image_folder = image_folder\n",
    "        self.processor = processor\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((384, 384)),\n",
    "            T.ColorJitter(brightness=0.05, contrast=0.05),\n",
    "            T.RandomHorizontalFlip(p=0.2),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_id = row['image_id']\n",
    "        caption = clean_caption(row['caption'])\n",
    "        image_path = os.path.join(self.image_folder, f\"{image_id}.jpg\")\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        inputs = self.processor(images=image, text=caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "        return {\n",
    "            'pixel_values': inputs['pixel_values'].squeeze(0),\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'caption': caption,\n",
    "            'image_id': image_id\n",
    "        }\n",
    "\n",
    "\n",
    "def compute_cider_score(preds, gts):\n",
    "    scorer = Cider()\n",
    "    score, _ = scorer.compute_score(gts, preds)\n",
    "    return score\n",
    "\n",
    "csv_path = # path of csv file included ID numbers of the images to be used for training\n",
    "image_folder = # path of the training images\n",
    "model_save_path = \"/content/drive/MyDrive/blip_finetuned\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "model_name = \"Salesforce/blip-image-captioning-base\"\n",
    "processor = BlipProcessor.from_pretrained(model_name)\n",
    "model = BlipForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "train_dataset = CaptionDataset(train_df, image_folder, processor)\n",
    "val_dataset = CaptionDataset(val_df, image_folder, processor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "num_epochs = 30\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=num_training_steps)\n",
    "loss_fn = CrossEntropyLoss(label_smoothing=0.1)\n",
    "scaler = GradScaler()\n",
    "\n",
    "print(\"‚úÖ Sƒ±fƒ±rdan eƒüitim ba≈ülatƒ±ldƒ±...\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nüìÜ Epoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    steps = 0\n",
    "    loop = tqdm(train_loader, desc=\"Train\", leave=False)\n",
    "\n",
    "    for batch in loop:\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        decoder_input_ids = input_ids[:, :-1]\n",
    "        labels = input_ids[:, 1:].detach().clone()\n",
    "        labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(pixel_values=pixel_values, input_ids=decoder_input_ids, attention_mask=attention_mask[:, :-1])\n",
    "            loss = loss_fn(outputs.logits.view(-1, outputs.logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        steps += 1\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / steps\n",
    "    print(f\"üìâ Avg Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    preds, gts = {}, {}\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            image_ids = batch['image_id']\n",
    "            captions = batch['caption']\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                pixel_values=pixel_values,\n",
    "                max_length=64,\n",
    "                num_beams=4,\n",
    "                decoder_start_token_id=processor.tokenizer.cls_token_id\n",
    "            )\n",
    "            generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "            for img_id, gt, pred in zip(image_ids, captions, generated_texts):\n",
    "                preds[img_id] = [pred.strip()]\n",
    "                gts[img_id] = [gt.strip()]\n",
    "\n",
    "    cider = compute_cider_score(preds, gts)\n",
    "    print(f\"üçè Validation CIDEr: {cider:.4f}\")\n",
    "\n",
    "    epoch_save_path = os.path.join(model_save_path, f\"epoch_{epoch+1}\")\n",
    "    os.makedirs(epoch_save_path, exist_ok=True)\n",
    "    model.save_pretrained(epoch_save_path)\n",
    "    processor.save_pretrained(epoch_save_path)\n",
    "\n",
    "print(\"\\nüèÅ Eƒüitim tamamlandƒ±.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gL0LBR6GyD9m"
   },
   "source": [
    "# **4- Eƒûƒ∞Tƒ∞ME DEVAM KODU**\n",
    "YUKARIDA Eƒûƒ∞Tƒ∞Lƒ∞P DRIVE'A KAYDEDƒ∞LEN MODELƒ∞N Eƒûƒ∞Tƒ∞Mƒ∞ BURADA DEVAM ETTƒ∞Rƒ∞LDƒ∞. BURADA CIDER SKORU D√ú≈û√úK GELENLER √úZERƒ∞NE YOƒûUNLA≈ûILDI VE KALƒ∞TE ARTTIRILDI. LR=1E-5 ƒ∞LE BA≈ûLADIKTAN 6 EPOCH SONRA LR DEƒûERƒ∞Nƒ∞ A≈ûAƒûIDAKƒ∞ DEƒûERE GETƒ∞RDƒ∞M VE 4 EPOCH DAHA Eƒûƒ∞TTƒ∞M. B√ñYLELƒ∞KLE OVERFIT OLMADAN D√úZG√úN Bƒ∞ √áALI≈ûMA YAPMAYA √áALI≈ûTIM.\n",
    "\n",
    "CONTINUE TRAINING\n",
    "\n",
    "THE TRAINING OF THE MODEL TRAINED ABOVE AND REGISTERED TO DRIVE WAS CONTINUED HERE. HERE, THE FOCUS WAS ON THOSE WITH LOW CIDER SCORES AND THE QUALITY WAS INCREASED. STARTING WITH LR=1E-5, AFTER 6 EPOCHES I BRING THE LR VALUE TO THE VALUE BELOW AND TRAINED 4 MORE EPOCHES. THUS I TRIED TO DO A PROPER WORK WITHOUT OVERFIT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7HCDhWzyCX-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, get_cosine_schedule_with_warmup\n",
    "from torch import torch, optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torchvision.transforms as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def clean_caption(caption):\n",
    "    caption = re.sub(r'[^\\x00-\\x7F]+', ' ', caption)\n",
    "    caption = re.sub(r'\\s+', ' ', caption).strip()\n",
    "    return caption\n",
    "\n",
    "def augment_caption(caption):\n",
    "    # Basit synonym replacement\n",
    "    synonyms = {\n",
    "        \"man\": \"guy\", \"woman\": \"lady\", \"dog\": \"puppy\",\n",
    "        \"cat\": \"kitten\", \"bike\": \"bicycle\", \"car\": \"vehicle\"\n",
    "    }\n",
    "    words = caption.split()\n",
    "    new_caption = [synonyms.get(word, word) for word in words]\n",
    "    return ' '.join(new_caption)\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, processor, augment=False):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.image_folder = image_folder\n",
    "        self.processor = processor\n",
    "        self.augment = augment\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((384, 384)),\n",
    "            T.RandomApply([T.RandomRotation(10)], p=0.3),\n",
    "            T.RandomApply([T.RandomCrop(384, pad_if_needed=True)], p=0.3),\n",
    "            T.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = str(self.df.iloc[idx]['image_id'])\n",
    "        caption = clean_caption(self.df.iloc[idx]['caption'])\n",
    "        if self.augment:\n",
    "            caption = augment_caption(caption)\n",
    "        image_path = os.path.join(self.image_folder, str(image_id) + \".jpg\")\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "\n",
    "        inputs = self.processor(\n",
    "            images=image,\n",
    "            text=caption,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=64\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'pixel_values': inputs['pixel_values'].squeeze(0),\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'caption': caption,\n",
    "            'image_id': image_id\n",
    "        }\n",
    "\n",
    "def compute_cider_score(preds, gts):\n",
    "    scorer = Cider()\n",
    "    score, _ = scorer.compute_score(gts, preds)\n",
    "    return score\n",
    "\n",
    "df = # pd.read_csv('path of train.csv file')\n",
    "image_folder = # path of the training images folder\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "model_path = \"/content/drive/MyDrive/blip_finetuned\"\n",
    "processor = BlipProcessor.from_pretrained(model_path)\n",
    "model = BlipForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "\n",
    "num_epochs = 30\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-6)\n",
    "loss_fn = CrossEntropyLoss(label_smoothing=0.1)\n",
    "scaler = GradScaler()\n",
    "num_training_steps = num_epochs * (len(train_df) // 16)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(optimizer, 500, num_training_steps)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nüìÜ Epoch {epoch + 1} / {num_epochs}\")\n",
    "    model.train()\n",
    "    train_dataset = CaptionDataset(train_df, image_folder, processor, augment=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    total_loss, steps = 0, 0\n",
    "    loop = tqdm(train_loader, desc=\"Train\", leave=False)\n",
    "\n",
    "    for batch in loop:\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        decoder_input_ids = input_ids[:, :-1]\n",
    "        labels = input_ids[:, 1:].clone()\n",
    "        labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        with autocast():\n",
    "          outputs = model(\n",
    "            pixel_values=pixel_values,\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=attention_mask[:, :-1]\n",
    "          )\n",
    "          logits = outputs.logits\n",
    "          loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        steps += 1\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / steps\n",
    "    print(f\"üìâ Epoch {epoch+1} - Avg Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    preds, gts, bad_samples = {}, {}, []\n",
    "    val_loader = DataLoader(CaptionDataset(val_df, image_folder, processor), batch_size=32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            image_ids = batch['image_id']\n",
    "            captions = batch['caption']\n",
    "\n",
    "            generated_ids = model.generate(pixel_values=pixel_values, max_length=64, num_beams=5)\n",
    "            generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "            for img_id, gt, pred in zip(image_ids, captions, generated_texts):\n",
    "                preds[img_id] = [pred.strip()]\n",
    "                gts[img_id] = [gt.strip()]\n",
    "                if pred.strip().lower() != gt.strip().lower():\n",
    "                    bad_samples.append({'image_id': img_id, 'caption': gt})\n",
    "\n",
    "    cider_score = compute_cider_score(preds, gts)\n",
    "    print(f\"üçè Validation CIDEr: {cider_score:.4f}\")\n",
    "\n",
    "    if bad_samples:\n",
    "        train_df = pd.concat([train_df, pd.DataFrame(bad_samples)], ignore_index=True)\n",
    "\n",
    "    model.save_pretrained(model_path)\n",
    "    processor.save_pretrained(model_path)\n",
    "\n",
    "print(\"\\nüèÅ Eƒüitim tamamlandƒ±.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whYp5kkbxyID"
   },
   "source": [
    "# **5- TAHMƒ∞N KODU**\n",
    "BURADAKƒ∞ KOD, TEST KLAS√ñR√úNDEKƒ∞ G√ñRSELLERE TAHMƒ∞N √úRETMEK ƒ∞√áƒ∞N KULLANILIR.\n",
    "\n",
    "**PREDICTION CODE**\n",
    "\n",
    "THE CODE HERE IS USED TO GENERATE PREDICTIONS FOR THE IMAGES IN THE TEST FOLDER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OnDwxYUI8uUc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dosya yollarƒ±\n",
    "test_csv_path = # path of the file included ID numbers of the images to be used for testing\n",
    "test_image_folder = # TEST IMAGES FOLDER PATH\n",
    "model_path = '/content/drive/MyDrive/blip_finetuned'\n",
    "\n",
    "test_df = pd.read_csv(test_csv_path)\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(model_path)\n",
    "model = BlipForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Generating Captions\"):\n",
    "    image_id = row['image_id']\n",
    "    image_path = os.path.join(test_image_folder, f\"{image_id}.jpg\")\n",
    "\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "    \n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            pixel_values=inputs['pixel_values'],\n",
    "            max_length=64,\n",
    "            num_beams=5,\n",
    "            repetition_penalty=2.0,\n",
    "            length_penalty=1.0,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    \n",
    "    caption = processor.tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    results.append({\n",
    "        \"image_id\": image_id,\n",
    "        \"caption\": caption\n",
    "    })\n",
    "\n",
    "\n",
    "submission_df = pd.DataFrame(results)\n",
    "submission_df.to_csv(\"/content/drive/MyDrive/tahminler.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Tahminler 'tahminler.csv' dosyasƒ±na kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfS5ndUHgEBB"
   },
   "source": [
    "# **6- TEK TAHMƒ∞N KODU**\n",
    "BURADAKƒ∞ KOD, SE√áƒ∞LEN G√ñRSELE TAHMƒ∞N √úRETMEK ƒ∞√áƒ∞N KULLANILIR.\n",
    "\n",
    "THE CODE HERE IS USED TO GENERATE A PREDICTION FOR THE SELECTED IMAGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GiAftyrggDMi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_path = 'blip_finetuned'\n",
    "processor = BlipProcessor.from_pretrained(model_path)\n",
    "model = BlipForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "model.eval()\n",
    "\n",
    "image_path = 'fotolar/balon.jpg'\n",
    "\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        pixel_values=inputs['pixel_values'],\n",
    "        max_length=64,\n",
    "        num_beams=5,\n",
    "        repetition_penalty=2.0,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "caption = processor.tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()\n",
    "print(f\"üñºÔ∏è Caption: {caption}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
